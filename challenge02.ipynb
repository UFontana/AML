{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nfrom torchaudio.backend.soundfile_backend import load\nfrom torch.utils.data import Dataset,DataLoader, ConcatDataset\nfrom torchaudio.transforms import MelSpectrogram\nimport torch.nn as nn\nimport torch \nimport numpy as np\nimport gc\nfrom tqdm import tqdm\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights, resnet50, ResNet50_Weights, ResNet18_Weights, resnet18\nimport torch.nn.functional as F\nimport math\nfrom torch import flatten\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\n#\n\nparams = {'eval': True,\n        'num_classes': 3,\n        'batch_size': 32,\n        'epochs': 1,\n        'win_len': 626, \n        'mel_bins': 128, \n        'hop_len': 313,\n        'n_fft' : 626,\n        'power' : 2.0,\n        'use_log_mel': False,\n        'arcface': True,\n        'net': 'STGramNet' # resnet50, SepSTGramNet, STGramNet\n       }","metadata":{"_uuid":"22165675-39fd-411a-9273-66fba58fa343","_cell_guid":"21b523fd-bf2b-45b8-aef0-299d3e54c6da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-30T17:41:31.337518Z","iopub.execute_input":"2023-05-30T17:41:31.337901Z","iopub.status.idle":"2023-05-30T17:41:31.346978Z","shell.execute_reply.started":"2023-05-30T17:41:31.337869Z","shell.execute_reply":"2023-05-30T17:41:31.346061Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### ArcFaceLoss","metadata":{}},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    def __init__(self, embed_size, num_classes, scale=64, margin=0.5, easy_margin=False, **kwargs):\n        super().__init__()\n        self.scale = scale\n        self.margin = margin\n        self.ce = nn.CrossEntropyLoss()\n        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embed_size))\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, embedding: torch.Tensor, ground_truth):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cos_theta = F.linear(F.normalize(embedding), F.normalize(self.weight)).clamp(-1 + 1e-7, 1 - 1e-7)\n        sin_theta = torch.sqrt((1.0 - torch.pow(cos_theta, 2)).clamp(-1 + 1e-7, 1 - 1e-7))\n        phi = cos_theta * self.cos_m - sin_theta * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cos_theta > 0, phi, cos_theta)\n        else:\n            phi = torch.where(cos_theta > self.th, phi, cos_theta - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cos_theta.size(), device='cuda')\n        one_hot.scatter_(1, ground_truth.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + (\n                (1.0 - one_hot) * cos_theta)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.scale\n\n        loss = self.ce(output, ground_truth)\n        return loss, output","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:41:32.537903Z","iopub.execute_input":"2023-05-30T17:41:32.538806Z","iopub.status.idle":"2023-05-30T17:41:32.553203Z","shell.execute_reply.started":"2023-05-30T17:41:32.538770Z","shell.execute_reply":"2023-05-30T17:41:32.552207Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## SepSTGramNet","metadata":{}},{"cell_type":"code","source":"class TGramNet(nn.Module):\n    def __init__(self, mel_bins,win_len, hop_len, num_layers=3, **kwargs):\n        super(TGramNet, self).__init__()\n        \n        self.conv1d = nn.Conv1d(in_channels=1, out_channels=mel_bins, kernel_size=win_len, stride=hop_len, padding=win_len // 2, bias=False)\n        self.conv_encoder = nn.Sequential(\n            *[nn.Sequential(\n                nn.LayerNorm(int(160000//hop_len)+1),\n                nn.GELU(),\n                nn.Conv1d(mel_bins, mel_bins, 3, 1, 1, bias=False)\n            ) for _ in range(num_layers)]\n        )\n        \n    def forward(self, x_w):\n        x_w = self.conv1d(x_w)\n        x_w = self.conv_encoder(x_w)\n        return x_w\n\nclass SepSTGramNet(nn.Module):\n    def __init__(self, params):\n        super(SepSTGramNet, self).__init__()\n        self.tgramnet = TGramNet(**params)\n        \n        self.t_resnet=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        for p in self.t_resnet.parameters():\n            p.requires_grad = True\n        self.t_resnet.fc=nn.Linear(in_features=512, out_features=384, bias=True)\n        \n        self.s_resnet=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        for p in self.s_resnet.parameters():\n            p.requires_grad = True\n        self.s_resnet.fc=nn.Linear(in_features=512, out_features=384, bias=True)\n        \n        if not params[\"arcface\"]:\n            self.fc=nn.Sequential(nn.Linear(in_features=384*2, out_features=70),   \n                               nn.BatchNorm1d(70),\n                               nn.GELU(),\n                               nn.Linear(in_features=70, out_features=params[\"num_classes\"]))\n        else:\n            self.fc=nn.Sequential(nn.Linear(in_features=384*2, out_features=70),   \n                               nn.BatchNorm1d(70),\n                               nn.GELU())\n            \n    def forward(self, x_wav, x_mel):\n        x_t = self.tgramnet(x_wav).unsqueeze(1).repeat(1, 3, 1, 1)\n        x_t = self.t_resnet(x_t)\n        x_mel = self.s_resnet(x_mel)\n        \n        if params['use_log_mel']:\n            x_log_mel = 20.0 / params[\"power\"] * torch.log10(x_mel + sys.float_info.epsilon)\n            x_log_mel = self.l_resnet(x_log_mel)\n\n            x = torch.cat((x_mel, x_mel, x_log_mel), dim=1).to(device) \n        else:\n            x = torch.cat((x_mel, x_mel), dim=1).to(device) \n        \n        out = self.fc(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:41:32.640865Z","iopub.execute_input":"2023-05-30T17:41:32.641218Z","iopub.status.idle":"2023-05-30T17:41:32.658506Z","shell.execute_reply.started":"2023-05-30T17:41:32.641188Z","shell.execute_reply":"2023-05-30T17:41:32.657405Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## STGramNet","metadata":{}},{"cell_type":"code","source":"class TGramNet(nn.Module):\n    def __init__(self, mel_bins,win_len, hop_len, num_layers=3, **kwargs):\n        super(TGramNet, self).__init__()\n        \n        self.conv1d = nn.Conv1d(in_channels=1, out_channels=mel_bins, kernel_size=win_len, stride=hop_len, padding=win_len // 2, bias=False)\n        self.conv_encoder = nn.Sequential(\n            *[nn.Sequential(\n                nn.LayerNorm(int(160000//hop_len)+1),\n                nn.GELU(),\n                nn.Conv1d(mel_bins, mel_bins, 3, 1, 1, bias=False)\n            ) for _ in range(num_layers)]\n        )\n        \n    def forward(self, x_w):\n        x_w = self.conv1d(x_w)\n        x_w = self.conv_encoder(x_w)\n        return x_w\n\nclass STGramNet(nn.Module):\n    def __init__(self, params):\n        super(STGramNet, self).__init__()\n        self.tgramnet = TGramNet(**params)\n        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        if not params[\"arcface\"]:\n            self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, params[\"num_classes\"])\n        else:\n            self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n            \n    def forward(self, x_wav, x_mel, label=None):\n        x_t = self.tgramnet(x_wav).unsqueeze(1)\n        zeros = torch.zeros_like(x_t, requires_grad=True).float()\n        x = torch.cat((x_mel, x_t, zeros), dim=1).to(device) \n        out = self.resnet(x)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:41:39.295843Z","iopub.execute_input":"2023-05-30T17:41:39.296217Z","iopub.status.idle":"2023-05-30T17:41:39.310100Z","shell.execute_reply.started":"2023-05-30T17:41:39.296188Z","shell.execute_reply":"2023-05-30T17:41:39.308554Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Model definition","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif params['net'] == 'resnet50':\n    net = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n    for p in net.parameters():\n        p.requires_grad = False\n\n    if not params[\"arcface\"]:\n        net.fc = torch.nn.Linear(net.fc.in_features, params[\"num_classes\"])\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(net.fc.parameters(), lr=1e-3)\n    else:\n        net = torch.nn.Sequential(*(list(net.children())[:-1]))\n        criterion = ArcFace(2048, params[\"num_classes\"], scale=2, margin=0.1).to(device)\n        optimizer = torch.optim.Adam(criterion.parameters(), lr=1e-3)\n\nelif params['net'] == 'SepSTGramNet':\n    net = SepSTGramNet(params)\n    if not params[\"arcface\"]:\n        optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n        criterion = torch.nn.CrossEntropyLoss()\n    else:\n        criterion = ArcFace(70, params[\"num_classes\"], scale=2, margin=0.1).to(device)        \n        net_params = [{'params': net.parameters()}, {'params': criterion.parameters()}]        \n        optimizer = torch.optim.Adam(net_params, lr=1e-3)\n\nelif params[\"net\"] == \"STGramNet\":\n    net = STGramNet(params)\n    if not params[\"arcface\"]:\n        optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n        criterion = torch.nn.CrossEntropyLoss()\n    else:\n        criterion = ArcFace(512, params[\"num_classes\"], scale=2, margin=0.1).to(device)        \n        net_params = [{'params': net.parameters()}, {'params': criterion.parameters()}]        \n        optimizer = torch.optim.Adam(net_params, lr=1e-3)\n        \nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:41:42.357494Z","iopub.execute_input":"2023-05-30T17:41:42.358288Z","iopub.status.idle":"2023-05-30T17:41:42.714195Z","shell.execute_reply.started":"2023-05-30T17:41:42.358244Z","shell.execute_reply":"2023-05-30T17:41:42.713223Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, dir_path, params, dev=True, audio_transform=None):\n        self.dir_path = dir_path\n        self.files = os.listdir(dir_path)\n        self.audio_transform = audio_transform\n        self.dev = dev\n        self.params = params\n        \n        \n    def __len__(self):\n        return len(self.files)\n\n    \n    def __getitem__(self, idx):\n        filename = self.files[idx]\n        path = self.dir_path + '/' + filename\n        machine_id = path.split('/')[-1].split('_')[-2]\n        label_path = path.split('/')[-1].split('_')[0]\n        \n        if label_path == \"normal\":\n            anomaly_label = 1\n        elif label_path == \"anomaly\":\n            anomaly_label = 0\n        else:\n            anomaly_label = 0\n            \n        audio, sr = load(path)\n        mel_spectrogram = MelSpectrogram(sample_rate=sr, n_fft=params['n_fft'], hop_length=params['hop_len'], power=params[\"power\"])\n        mel_spectr = mel_spectrogram(audio)\n        if self.audio_transform is not None:\n            audio = self.audio_transform(audio)\n        return audio, mel_spectr, self._return_one_hot(machine_id), anomaly_label, filename\n    \n    \n    def _return_one_hot(self, machine_id):\n        machine_id = int(machine_id)\n        if not self.dev:\n            machine_id -= 1\n        machine_id //= 2\n        if self.params[\"arcface\"]:\n            return torch.tensor(machine_id).long()\n        t = np.zeros(self.params[\"num_classes\"])\n        t[machine_id] = 1\n        return torch.tensor(t).float()\n\n    \n# load datasets    \n\ndev_train_path = \"/kaggle/input/eurecom-aml-2023-challenge-2/dev_data/dev_data/slider/train\"\ndev_test_path = \"/kaggle/input/eurecom-aml-2023-challenge-2/dev_data/dev_data/slider/test\"\neval_train_path = \"/kaggle/input/eurecom-aml-2023-challenge-2/eval_data/eval_data/slider/train\"\neval_test_path = \"/kaggle/input/eurecom-aml-2023-challenge-2/eval_data/eval_data/slider/test\"\n\ndev_train_ds = AudioDataset(dev_train_path, params)\neval_train_ds = AudioDataset(eval_train_path, params, dev=False)\ndev_test_ds = AudioDataset(dev_test_path, params)\neval_test_ds = AudioDataset(eval_test_path, params, dev=False)\n\ndev_train_dl = DataLoader(dev_train_ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=2)\ndev_test_dl = DataLoader(dev_test_ds, batch_size=params[\"batch_size\"])\neval_train_dl = DataLoader(eval_train_ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=2)\neval_test_dl = DataLoader(eval_test_ds, batch_size=params[\"batch_size\"])","metadata":{"_uuid":"2ab11d80-9e9e-4505-b01c-894efad2df2a","_cell_guid":"d9c40d33-77cd-4466-8940-5e7645cd6d9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-30T17:41:51.341603Z","iopub.execute_input":"2023-05-30T17:41:51.342005Z","iopub.status.idle":"2023-05-30T17:41:51.369734Z","shell.execute_reply.started":"2023-05-30T17:41:51.341975Z","shell.execute_reply":"2023-05-30T17:41:51.368808Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## DEV - Train","metadata":{}},{"cell_type":"code","source":"if not params[\"eval\"]:\n    net.train()\n    l = []\n    for e in range(params[\"epochs\"]):\n        print(f\"epoch {e}th\")\n        epoch_loss = []\n        for audio, spectrogram, machine_id, label, filename in tqdm(dev_train_dl):\n            optimizer.zero_grad()\n            if params[\"net\"] == \"SepSTGramNet\":\n                output = net(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze() \n            elif params[\"net\"] == \"STGramNet\":\n                output = net(audio.to(device), spectrogram.to(device)).squeeze() \n            else:\n                output = net(spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze()\n            if params[\"arcface\"]:\n                loss, _ = criterion(output, machine_id.to(device))\n            else:\n                loss = criterion(output, machine_id.to(device))\n            loss.backward()\n            optimizer.step()\n            epoch_loss.append(loss.item())\n        l.append(np.array(epoch_loss).mean())\n        print(np.array(epoch_loss).mean())\n    print('done')\n\n    plt.plot(l)","metadata":{"_uuid":"e9c41679-8257-4951-8f93-4c3fabd5804e","_cell_guid":"ce7001c8-6eb0-44d7-b347-6a8bed9a86b7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-30T17:41:51.371455Z","iopub.execute_input":"2023-05-30T17:41:51.371811Z","iopub.status.idle":"2023-05-30T17:41:51.384786Z","shell.execute_reply.started":"2023-05-30T17:41:51.371779Z","shell.execute_reply":"2023-05-30T17:41:51.383535Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## DEV - Test","metadata":{}},{"cell_type":"code","source":"if not params[\"eval\"]:\n    net.eval()\n    names = []\n    scores = []\n    labels = []\n    with torch.no_grad():\n        for audio, spectrogram, machine_id, label, filename in tqdm(dev_test_dl):\n            if not params[\"arcface\"]:\n                if params[\"net\"] == \"SepSTGramNet\":\n                    output = net.forward(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1))\n                elif params[\"net\"] == \"STGramNet\":\n                    output = net.forward(audio.to(device), spectrogram.to(device))\n                else:\n                    output = net.forward(spectrogram.to(device).repeat(1, 3, 1, 1))\n                softmax = output.softmax(dim=1)\n                score = softmax[:, machine_id.to(device).argmax(dim=1)].diag()\n                # prob = 1 - score\n            else:\n                if params[\"net\"] == \"SepSTGramNet\":\n                    _, output = criterion(net.forward(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1)), machine_id.to(device))\n                elif params[\"net\"] == \"STGramNet\":\n                    _, output = criterion(net.forward(audio.to(device), spectrogram.to(device)).squeeze(2, 3), machine_id.to(device))\n                else:\n                    _, output = criterion(net.forward(spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze(2, 3), machine_id.to(device))\n                softmax = output.softmax(dim=1)\n                score = softmax[:, machine_id.to(device)].diag()\n            names.extend(filename)\n            scores.extend(score.cpu().detach().numpy())\n            labels.extend(label.cpu().detach().numpy())\n            \n    print(roc_auc_score(labels, scores))","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:41:51.386354Z","iopub.execute_input":"2023-05-30T17:41:51.386778Z","iopub.status.idle":"2023-05-30T17:41:51.401088Z","shell.execute_reply.started":"2023-05-30T17:41:51.386744Z","shell.execute_reply":"2023-05-30T17:41:51.399931Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## EVAL - Train and Test","metadata":{}},{"cell_type":"code","source":"if params[\"eval\"]:\n    \n    # train\n    net.train()\n    l = []\n    for e in range(params[\"epochs\"]):\n        print(f\"epoch {e}th\")\n        epoch_loss = []\n        for audio, spectrogram, machine_id, label, filename in tqdm(eval_train_dl):\n            optimizer.zero_grad()\n            if params[\"net\"] == \"SepSTGramNet\":\n                output = net(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze()\n            elif params[\"net\"] == \"STGramNet\":\n                output = net(audio.to(device), spectrogram.to(device))\n            else:\n                output = net(spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze()\n            if params[\"arcface\"]:\n                if params[\"net\"] == \"STGramNet\":\n                    loss, _ = criterion(output.squeeze(2, 3), machine_id.to(device))\n                else:\n                    loss, _ = criterion(output, machine_id.to(device))\n            else:\n                loss = criterion(output, machine_id.to(device))\n            loss.backward()\n            optimizer.step()\n            epoch_loss.append(loss.item())\n        l.append(np.array(epoch_loss).mean())\n        print(np.array(epoch_loss).mean())\n    print('done')\n    \n    # eval\n    net.eval()\n    names = []\n    scores = []\n    labels = []\n    with torch.no_grad():\n        for audio, spectrogram, machine_id, label, filename in tqdm(eval_test_dl):\n            if not params[\"arcface\"]:\n                if params[\"net\"] == \"SepSTGramNet\":\n                    output = net.forward(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1))\n                elif params[\"net\"] == \"STGramNet\":\n                    output = net.forward(audio.to(device), spectrogram.to(device))\n                else:\n                    output = net.forward(spectrogram.to(device).repeat(1, 3, 1, 1))\n                softmax = output.softmax(dim=1)\n                score = softmax[:, machine_id.to(device).argmax(dim=1)].diag()\n            else:\n                if params[\"net\"] == \"SepSTGramNet\":\n                    _, output = criterion(net.forward(audio.to(device), spectrogram.to(device).repeat(1, 3, 1, 1)), machine_id.to(device))\n                elif params[\"net\"] == \"STGramNet\":\n                    _, output = criterion(net.forward(audio.to(device), spectrogram.to(device)).squeeze(2, 3), machine_id.to(device))\n                else:\n                    _, output = criterion(net.forward(spectrogram.to(device).repeat(1, 3, 1, 1)).squeeze(2, 3), machine_id.to(device))\n                softmax = output.softmax(dim=1)\n                score = softmax[:, machine_id.to(device)].diag()\n            \n            prob = 1 - score\n            names.extend(filename)\n            scores.extend(prob.cpu().detach().numpy())\n            labels.extend(label.cpu().detach().numpy())\n            \n    submission_df = pd.DataFrame(names, columns=[\"file_name\"])\n    submission_df[\"anomaly_score\"] = scores\n    submission_df.to_csv(\"/kaggle/working/sample_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T17:45:14.794281Z","iopub.execute_input":"2023-05-30T17:45:14.795162Z","iopub.status.idle":"2023-05-30T17:45:51.371942Z","shell.execute_reply.started":"2023-05-30T17:45:14.795120Z","shell.execute_reply":"2023-05-30T17:45:51.370571Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"epoch 0th\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 75/75 [00:21<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"0.268907280365626\ndone\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 27/27 [00:14<00:00,  1.82it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"5 epochs\nmel: 0.8461880982105701\nlog_mel: 0.7744319600499376\n\n10 epochs\nmel: 0.8667582188930503\nlog_mel: 0.7555222638368705\n\n20 epochs\nmel: 0.8175343320848939\nlog_mel: 0.7449105285060342\n\nArcFaceLoss\nbatch_size=32, epochs=10, s=2, m=0.1\n0.9132792342904703","metadata":{}}]}